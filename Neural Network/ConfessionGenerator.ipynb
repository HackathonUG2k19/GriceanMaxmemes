{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConfessionGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2mE7r1XI_K4",
        "colab_type": "text"
      },
      "source": [
        "# `The Neural Network`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkumZNmuBS3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWeMakG8YTtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "rm -rf training_checkpoints\n",
        "rm -rf Confessions-Generator\n",
        "rm text_data.txt\n",
        "git clone https://github.com/AurumnPegasus/Confessions-Generator.git\n",
        "unzip Confessions-Generator/DataSet/text_data.txt.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feerEKM5Gwsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Netowrk Requirements\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Uitlities\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDgYNTpnQWmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NN Requirements\n",
        "# path to data file\n",
        "path_to_file = \"text_data.txt\"\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "# Batch size\n",
        "BATCH_SIZE = 256\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "# Number of RNN units here, LSTM units are used\n",
        "rnn_units = 1024\n",
        "# Total number of times the model sees the data\n",
        "EPOCHS = 5\n",
        "\n",
        "# Databse Requirements\n",
        "# total confessions\n",
        "database_size = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEJqN4vSG88X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the file in utf-8 format\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# length of text = number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM1WnPbtG_J1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[500:1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou34GjGMHD6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKFpBvpXHGjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {c:i for i, c in enumerate(vocab)}\n",
        "# Creating a reverse mapping as well\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# Convert the entire text to their encodings\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuSKn0vjHO9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# +1 because we shift by 1 character each time\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOk7eW89HWne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCopxIPVHZy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    # splits the input chunk\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxNPMVHUHjyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shuffle the dataset\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LLIAj-OHlu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnIAXlIzHnlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  \"\"\"\n",
        "  Builds the Sequential Model, using Embeddings to create inputs to tensors of specified dimensions, and then find dependencies using LSTM units. At the end is the dense layer.\n",
        "\n",
        "  input params:\n",
        "  vocab_size: nubmer of unique characters\n",
        "  embedding_dim: number of dimension in the embedding tensor\n",
        "  rnn_units: self explanatory\n",
        "  batch_size: number of samples seen before gradient updation\n",
        "\n",
        "  output:\n",
        "  model created\n",
        "  \"\"\"\n",
        "  \n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5TVrtrzHoxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIpB1joSHr45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxE_oH8NEglg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5GU_RyDH6dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=custom_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Z4h-XdJTEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy67g_xOIFzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=checkpoint_callback)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYXqgTVGKmQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uCdTTInJGCe",
        "colab_type": "text"
      },
      "source": [
        "# `Database Generation`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9mB4WfmI2FN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pbWOMSYKYLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, out_size, start_string):\n",
        "    # Number of characters to generate\n",
        "    num_generate = out_size\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperatures results in more predictable text.\n",
        "    # Higher temperatures results in more surprising text.\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlAfI5AVM9ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generate_text(model, 100, \"My \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycd5MtZwKaeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "database = [[], [], []]\n",
        "\n",
        "for _ in tqdm_notebook(range(database_size)):\n",
        "\n",
        "    # random size\n",
        "    user_jo_bola = random.randint(60, 420)\n",
        "    temp_conf_string = generate_text(model, user_jo_bola, start_string=\"Dear\")\n",
        "    index = temp_conf_string.rfind('.')\n",
        "    conf_string = temp_conf_string[:index] + \".\"\n",
        "\n",
        "    if user_jo_bola < 180:\n",
        "        # relative_conf = \"smallpp\"\n",
        "        database[0].append(conf_string)\n",
        "    elif user_jo_bola < 300:\n",
        "        # relative_conf = \"mediumpp\"\n",
        "        database[1].append(conf_string)\n",
        "    else:\n",
        "        # relative_conf = \"largepp\"\n",
        "        database[2].append(conf_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPmRzC9URnKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pp_sizes = [\"smallpp\", \"mediumpp\", \"largepp\"]\n",
        "\n",
        "for i, name in enumerate(pp_sizes):\n",
        "    with open(f\"/content/drive/My Drive/Database/{name}_database.json\", 'w', encoding='utf-8') as f:\n",
        "        json.dump({\"content\":database[i]}, f, ensure_ascii=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}